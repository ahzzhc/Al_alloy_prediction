#!/usr/bin/python
# -*- coding: UTF-8 -*-

# Train data using DNN/ResNet

from keras.models import Sequential,Model
from keras.layers import Dense, Dropout,BatchNormalization,Input,add # , LSTM, Activation
from keras.optimizers import Adam, Adadelta
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np
import time

data = np.load('Al_energy118_split.npz',allow_pickle=True)
train_x=data["train_x"]
train_y=data["train_y"]
test_x=data["test_x"]
test_y=data["test_y"]

# DNN model training
# model1
time_start1=time.time()
model1 = Sequential()
model1.add(Dense(118, init='uniform', activation='relu',input_dim=train_x.shape[1]))
model1.add(BatchNormalization())
model1.add(Dense(64, activation='relu'))
model1.add(Dense(32, activation='relu'))
model1.add(Dense(32, activation='relu'))
model1.add(Dense(32, activation='relu'))
model1.add(Dropout(0.25))
model1.add(Dense(1, activation='linear'))
adamoptimizer1 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model1.compile(optimizer=adamoptimizer1, loss='mae')
history1= model1.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y1 = model1.predict(test_x, batch_size=1)
loss1=model1.evaluate(test_x, test_y) # , batch_size=10)
time_end1=time.time()
print('model1 totally cost time:',time_end1-time_start1)
model1_RS = 1 - (mean_squared_error(test_y,predict_y1)/np.var(test_y))
print(model1_RS)
model1_MAE=mean_absolute_error(test_y,predict_y1)
print(model1_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('DNN Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y1,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model2
time_start2=time.time()
model2 = Sequential()
model2.add(Dense(118, init='uniform', activation='relu',input_dim=train_x.shape[1]))
model2.add(BatchNormalization())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(64, activation='relu'))
model2.add(Dense(32, activation='relu'))
model2.add(Dense(32, activation='relu'))
model2.add(Dense(32, activation='relu'))
model2.add(Dropout(0.25))
model2.add(Dense(1, activation='linear'))
adamoptimizer2 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model2.compile(optimizer=adamoptimizer2, loss='mae')
history2= model2.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y2 = model2.predict(test_x, batch_size=1)
loss2=model2.evaluate(test_x, test_y) # , batch_size=10)
time_end2=time.time()
print('model2 totally cost time:',time_end2-time_start2)
model2_RS = 1 - (mean_squared_error(test_y,predict_y2)/np.var(test_y))
print(model2_RS)
model2_MAE=mean_absolute_error(test_y,predict_y2)
print(model2_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('DNN Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y2,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model3
time_start3=time.time()
model3 = Sequential()
model3.add(Dense(118, init='uniform', activation='relu',input_dim=train_x.shape[1]))
model3.add(BatchNormalization())
model3.add(Dense(128, activation='relu'))
model3.add(Dense(128, activation='relu'))
model3.add(Dense(64, activation='relu'))
model3.add(Dense(32, activation='relu'))
model3.add(Dense(32, activation='relu'))
model3.add(Dense(32, activation='relu'))
model3.add(Dropout(0.25))
model3.add(Dense(1, activation='linear'))
adamoptimizer3 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model3.compile(optimizer=adamoptimizer3, loss='mae')
history3= model3.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y3 = model3.predict(test_x, batch_size=1)
loss3=model3.evaluate(test_x, test_y) # , batch_size=10)
time_end3=time.time()
print('model3 totally cost time:',time_end3-time_start3)
model3_RS = 1 - (mean_squared_error(test_y,predict_y3)/np.var(test_y))
print(model3_RS)
model3_MAE=mean_absolute_error(test_y,predict_y3)
print(model3_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('DNN Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y3,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model4
time_start4=time.time()
model4 = Sequential()
model4.add(Dense(118, init='uniform', activation='relu',input_dim=train_x.shape[1]))
model4.add(BatchNormalization())
model4.add(Dense(256, activation='relu'))
model4.add(Dense(256, activation='relu'))
model4.add(Dense(128, activation='relu'))
model4.add(Dense(128, activation='relu'))
model4.add(Dense(64, activation='relu'))
model4.add(Dense(32, activation='relu'))
model4.add(Dense(32, activation='relu'))
model4.add(Dense(32, activation='relu'))
model4.add(Dropout(0.25))
model4.add(Dense(1, activation='linear'))
adamoptimizer4 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model4.compile(optimizer=adamoptimizer4, loss='mae')
history4= model4.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y4 = model4.predict(test_x, batch_size=1)
loss4=model4.evaluate(test_x, test_y) # , batch_size=10)
time_end4=time.time()
print('model4 totally cost time:',time_end4-time_start4)
model4_RS = 1 - (mean_squared_error(test_y,predict_y4)/np.var(test_y))
print(model4_RS)
model4_MAE=mean_absolute_error(test_y,predict_y4)
print(model4_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('DNN Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y4,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

# Summarize the results of the three models
epochs1 = len(history1.history['loss'])
plt.plot(range(epochs1), history1.history['val_loss'], label='32*3',linestyle='--')
epochs2 = len(history2.history['loss'])
plt.plot(range(epochs2), history2.history['val_loss'], label='64+32*3',linestyle='--')
epochs3 = len(history3.history['loss'])
plt.plot(range(epochs3), history3.history['val_loss'], label='128*2+64+32*3',linestyle='--')
epochs4 = len(history4.history['loss'])
plt.plot(range(epochs4), history4.history['val_loss'], label='256*2+128*2+64+32*3',linestyle='--')
plt.ylabel('MAE', fontname='Arial Narrow', size=28)
plt.xlabel('Epoch', fontname='Arial Narrow', size=28)
plt.ylim(0, 1)
plt.legend()
plt.show()


