#!/usr/bin/python
# -*- coding: UTF-8 -*-

# Train data using DNN/ResNet

from keras.models import Sequential,Model
from keras.layers import Dense, Dropout,BatchNormalization,Input,add # , LSTM, Activation
from keras.optimizers import Adam, Adadelta
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np
import time

data = np.load('Al_energy118_split.npz',allow_pickle=True)
train_x=data["train_x"]
train_y=data["train_y"]
test_x=data["test_x"]
test_y=data["test_y"]

# DNN model training
# model1
time_start1=time.time()
input_layer = Input(shape=(train_x.shape[1], ))
dense1 = Dense(118, init='uniform', activation='relu')(input_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(128, activation='relu')(dense1)
dense3 = Dense(118,  activation='relu')(dense2)
z1 = add([dense1, dense3])
dense4 = Dense(32, activation='relu')(z1)
dense5 = Dense(32, activation='relu')(dense4)
dense6 = Dense(32, activation='relu')(dense5)
dense6 = Dropout(0.25)(dense6)
out_layer = Dense(1,  activation='linear')(dense6)
model1 = Model(inputs=input_layer, outputs=out_layer)
adamoptimizer1 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model1.compile(optimizer=adamoptimizer1, loss='mae')
history1= model1.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y1 = model1.predict(test_x, batch_size=1)
loss1=model1.evaluate(test_x, test_y) # , batch_size=10)
time_end1=time.time()
print('model1 totally cost time:',time_end1-time_start1)
model1_RS = 1 - (mean_squared_error(test_y,predict_y1)/np.var(test_y))
print(model1_RS)
model1_MAE=mean_absolute_error(test_y,predict_y1)
print(model1_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('ResNet Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y1,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model2
time_start2=time.time()
input_layer = Input(shape=(train_x.shape[1], ))
dense1 = Dense(118, init='uniform', activation='relu')(input_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(128, activation='relu')(dense1)
dense3 = Dense(128, activation='relu')(dense2)
dense4 = Dense(118,  activation='relu')(dense3)
z1 = add([dense1, dense4])
dense5 = Dense(32, activation='relu')(z1)
dense6 = Dense(32, activation='relu')(dense5)
dense7 = Dense(32, activation='relu')(dense6)
dense7 = Dropout(0.25)(dense7)
out_layer = Dense(1,  activation='linear')(dense7)
model2 = Model(inputs=input_layer, outputs=out_layer)
adamoptimizer2 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model2.compile(optimizer=adamoptimizer2, loss='mae')
history2= model2.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y2 = model2.predict(test_x, batch_size=1)
loss2=model2.evaluate(test_x, test_y) # , batch_size=10)
time_end2=time.time()
print('model2 totally cost time:',time_end2-time_start2)
model2_RS = 1 - (mean_squared_error(test_y,predict_y2)/np.var(test_y))
print(model2_RS)
model2_MAE=mean_absolute_error(test_y,predict_y2)
print(model2_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('ResNet Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y2,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model3
time_start3=time.time()
input_layer = Input(shape=(train_x.shape[1], ))
dense1 = Dense(118, init='uniform', activation='relu')(input_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(256, activation='relu')(dense1)
dense3 = Dense(128, activation='relu')(dense2)
dense4 = Dense(128, activation='relu')(dense3)
dense5 = Dense(118,  activation='relu')(dense4)
z1 = add([dense1, dense5])
dense6 = Dense(32, activation='relu')(z1)
dense7 = Dense(32, activation='relu')(dense6)
dense8 = Dense(32, activation='relu')(dense7)
dense8 = Dropout(0.25)(dense8)
out_layer = Dense(1,  activation='linear')(dense8)
model3 = Model(inputs=input_layer, outputs=out_layer)
adamoptimizer3 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model3.compile(optimizer=adamoptimizer3, loss='mae')
history3= model3.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y3 = model3.predict(test_x, batch_size=1)
loss3=model3.evaluate(test_x, test_y) # , batch_size=10)
time_end3=time.time()
print('model3 totally cost time:',time_end3-time_start3)
model3_RS = 1 - (mean_squared_error(test_y,predict_y3)/np.var(test_y))
print(model3_RS)
model3_MAE=mean_absolute_error(test_y,predict_y3)
print(model3_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('ResNet Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y3,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

#model4
time_start4=time.time()
input_layer = Input(shape=(train_x.shape[1], ))
dense1 = Dense(118, init='uniform', activation='relu')(input_layer)
dense1 = BatchNormalization()(dense1)
dense2 = Dense(256, activation='relu')(dense1)
dense3 = Dense(256, activation='relu')(dense2)
dense4 = Dense(128, activation='relu')(dense3)
dense5 = Dense(128, activation='relu')(dense4)
dense6 = Dense(118,  activation='relu')(dense5)
z1 = add([dense1, dense6])
dense7 = Dense(32, activation='relu')(z1)
dense8 = Dense(32, activation='relu')(dense7)
dense9 = Dense(32, activation='relu')(dense8)
dense9 = Dropout(0.25)(dense9)
out_layer = Dense(1,  activation='linear')(dense9)
model4 = Model(inputs=input_layer, outputs=out_layer)
adamoptimizer4 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.00001)
model4.compile(optimizer=adamoptimizer4, loss='mae')
history4= model4.fit(train_x, train_y, epochs=300, batch_size=256,validation_data=(test_x, test_y))
predict_y4 = model4.predict(test_x, batch_size=1)
loss4=model4.evaluate(test_x, test_y) # , batch_size=10)
time_end4=time.time()
print('model4 totally cost time:',time_end4-time_start4)
model4_RS = 1 - (mean_squared_error(test_y,predict_y4)/np.var(test_y))
print(model4_RS)
model4_MAE=mean_absolute_error(test_y,predict_y4)
print(model4_MAE)
plt.figure(figsize=(6,6))
plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)
plt.rc('font', family='Arial narrow')
plt.title('ResNet Model', fontsize=28, pad=12)
plt.tick_params(labelsize=26)
plt.ylabel('ML Prediction', fontname='Arial Narrow', size=28)
plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=28)
plt.scatter(test_y,predict_y4,c='orange',marker="*",edgecolors='dimgrey', alpha=1.0)
plt.plot(test_y,test_y)
plt.grid(False)
plt.show()

# Summarize the results of the three models
epochs1 = len(history1.history['loss'])
plt.plot(range(epochs1), history1.history['val_loss'], label='128+118+32*3',linestyle='--')
epochs2 = len(history2.history['loss'])
plt.plot(range(epochs2), history2.history['val_loss'], label='128*2+118+32*3',linestyle='--')
epochs3 = len(history3.history['loss'])
plt.plot(range(epochs3), history3.history['val_loss'], label='256+128*2+118+32*3',linestyle='--')
epochs4 = len(history4.history['loss'])
plt.plot(range(epochs4), history4.history['val_loss'], label='256*2+128*2+118+32*3',linestyle='--')
plt.ylabel('MAE', fontname='Arial Narrow', size=28)
plt.xlabel('Epoch', fontname='Arial Narrow', size=28)
plt.ylim(0, 1)
plt.legend()
plt.show()